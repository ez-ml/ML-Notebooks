{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Classification Demo\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "The goal of text classifcation is the classification of documents into a fixed number of predefined categories. In this notebook, we will walk through an example of text classifcation against a well known text classification data set using Spark machine learning algorithms. We will specifically classify the documents into two categories - a binary classification.\n",
    "\n",
    "## Spark MLlib\n",
    "\n",
    "MLlib is Spark’s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. It consists of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering, dimensionality reduction, as well as lower-level optimization primitives and higher-level pipeline APIs. In this notebook, we will explore how to employ feature extraction transformations and classification algorithms for document classification as well as how to combine these into a single pipeline or workflow. In this example, we will specifically utilize the spark.ml package of Spark MLlib as it provides a high-level API built on top of DataFrames, a Spark abstraction layer that provides for a distributed collection of data organized into named columns, for constructing ML pipelines.\n",
    "\n",
    "## Data Set\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering. In this demo, we will only use a subset of the 20 Newsgroups data set consisting of 2000 articles - 100 articles from each of the 20 newsgroups. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale / soc.religion.christian). Here is a list of the 20 newsgroups, partitioned (more or less) according to subject matter:\n",
    "  \n",
    "* comp.graphics\n",
    "* comp.os.ms-windows.misc\n",
    "* comp.sys.ibm.pc.hardware\n",
    "* comp.sys.mac.hardware\n",
    "* comp.windows.x\n",
    "* rec.autos\n",
    "* rec.motorcycles\n",
    "* rec.sport.baseball\n",
    "* rec.sport.hockey\n",
    "* sci.crypt\n",
    "* sci.electronics\n",
    "* sci.med\n",
    "* sci.space\n",
    "* misc.forsale\n",
    "* talk.politics.misc\n",
    "* talk.politics.guns\n",
    "* talk.politics.mideast\n",
    "* talk.religion.misc\n",
    "* alt.atheism\n",
    "* soc.religion.christian\n",
    "  \n",
    "Acknowledgement: Hettich, S. and Bay, S. D. (1999). The UCI KDD Archive [http://kdd.ics.uci.edu]. Irvine, CA: University of California, Department of Information and Computer Science.\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this exercise we are going to train a model to classify documents from the 20 Newsgroups data set into two categories according to whether or not the documents are computer related. We will then evaluate and tune the model against a test data set with documents that the model was not trained against.\n",
    "\n",
    "## One other thing to note\n",
    "\n",
    "In this notebook, investigation of the DataFrame objects is illustrated with both the DataFrame API and SQL - after first registering the DataFrame as a temporary table. Registering a DataFrame as a table allows you to run SQL queries over its data. Showing both DataFrame and SQL access is strictly done for illustrative purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data set\n",
    "#### Obtain a subset of the 20 Newsgroups data set\n",
    "A tarball of the 2000 document subset of the 20 Newsgroups data can be found at https://kdd.ics.uci.edu/databases/20newsgroups/mini_newsgroups.tar.gz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-10-18 19:39:54--  https://kdd.ics.uci.edu/databases/20newsgroups/mini_newsgroups.tar.gz\n",
      "Resolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.95\n",
      "Connecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.95|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1860687 (1.8M) [application/x-gzip]\n",
      "Saving to: 'mini_newsgroups.tar.gz'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  2%  798K 2s\n",
      "    50K .......... .......... .......... .......... ..........  5%  529K 3s\n",
      "   100K .......... .......... .......... .......... ..........  8%  783K 2s\n",
      "   150K .......... .......... .......... .......... .......... 11%  529K 3s\n",
      "   200K .......... .......... .......... .......... .......... 13%  789K 2s\n",
      "   250K .......... .......... .......... .......... .......... 16%  790K 2s\n",
      "   300K .......... .......... .......... .......... .......... 19%  527K 2s\n",
      "   350K .......... .......... .......... .......... .......... 22%  789K 2s\n",
      "   400K .......... .......... .......... .......... .......... 24%  786K 2s\n",
      "   450K .......... .......... .......... .......... .......... 27%  533K 2s\n",
      "   500K .......... .......... .......... .......... .......... 30%  786K 2s\n",
      "   550K .......... .......... .......... .......... .......... 33%  783K 2s\n",
      "   600K .......... .......... .......... .......... .......... 35%  790K 2s\n",
      "   650K .......... .......... .......... .......... .......... 38%  786K 2s\n",
      "   700K .......... .......... .......... .......... .......... 41%  536K 2s\n",
      "   750K .......... .......... .......... .......... .......... 44% 1.37M 1s\n",
      "   800K .......... .......... .......... .......... .......... 46%  554K 1s\n",
      "   850K .......... .......... .......... .......... .......... 49%  790K 1s\n",
      "   900K .......... .......... .......... .......... .......... 52% 1.43M 1s\n",
      "   950K .......... .......... .......... .......... .......... 55%  793K 1s\n",
      "  1000K .......... .......... .......... .......... .......... 57%  809K 1s\n",
      "  1050K .......... .......... .......... .......... .......... 60%  795K 1s\n",
      "  1100K .......... .......... .......... .......... .......... 63%  787K 1s\n",
      "  1150K .......... .......... .......... .......... .......... 66% 1.36M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 68%  842K 1s\n",
      "  1250K .......... .......... .......... .......... .......... 71%  796K 1s\n",
      "  1300K .......... .......... .......... .......... .......... 74% 1.45M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 77%  813K 1s\n",
      "  1400K .......... .......... .......... .......... .......... 79% 1.46M 0s\n",
      "  1450K .......... .......... .......... .......... .......... 82%  814K 0s\n",
      "  1500K .......... .......... .......... .......... .......... 85%  788K 0s\n",
      "  1550K .......... .......... .......... .......... .......... 88% 1.48M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 90%  804K 0s\n",
      "  1650K .......... .......... .......... .......... .......... 93% 1.53M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 96% 1.38M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 99%  834K 0s\n",
      "  1800K .......... .......                                    100% 52.0M=2.2s\n",
      "\n",
      "2016-10-18 19:39:57 (821 KB/s) - 'mini_newsgroups.tar.gz' saved [1860687/1860687]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\"rm -f mini_newsgroups.tar.gz\".!\n",
    "\"wget https://kdd.ics.uci.edu/databases/20newsgroups/mini_newsgroups.tar.gz\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explode the tarball\n",
    "\n",
    "The result is 20 directories corresponding to each of the 20 newsgroups topics. Each directory contains 100 documents stored as files according to topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"rm -rf mini_newsgroups\".!\n",
    "\"tar -zxf mini_newsgroups.tar.gz\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the resulting directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 alt.atheism\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 comp.graphics\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 comp.os.ms-windows.misc\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 comp.sys.ibm.pc.hardware\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 comp.sys.mac.hardware\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 comp.windows.x\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 misc.forsale\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 rec.autos\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 rec.motorcycles\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 rec.sport.baseball\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 rec.sport.hockey\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 sci.crypt\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 sci.electronics\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 sci.med\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 sci.space\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 soc.religion.christian\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 talk.politics.guns\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 talk.politics.mideast\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 talk.politics.misc\n",
      "drwxr-xr-x 2 s794-746ee4c5b0e6ab-a5f39cf201a0 users 4096 Apr 20  1997 talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "\"ls -l mini_newsgroups\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Setup\n",
    "#### Validate Spark context and create a Spark SQL context\n",
    "A Spark context is already defined in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version = 1.6.0\n",
      "Spark SQL context: org.apache.spark.sql.SQLContext@bca0929c\n"
     ]
    }
   ],
   "source": [
    "println(\"Spark version = \" + sc.version)\n",
    "val sqlContext= new org.apache.spark.sql.SQLContext(sc)\n",
    "println(\"Spark SQL context: \" + sqlContext)\n",
    "import sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required machine learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{HashingTF, StopWordsRemover, IDF, Tokenizer}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "import org.apache.spark.mllib.linalg.Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the newsgroups documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wholeTextFiles lets you read a directory structure containing multiple small text files and returns each of them as (filepath, content) pairs. We will do this for each topic and union the resulting RDDs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val path = \"mini_newsgroups/*\"\n",
    "val newsgroupsRawData = sc.wholeTextFiles(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Show a count of the number of documents read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents read in is 2000.\n"
     ]
    }
   ],
   "source": [
    "println(\"The number of documents read in is \" + newsgroupsRawData.count() + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at a sample (filepath, content) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s794-746ee4c5b0e6ab-a5f39cf201a0/notebook/work/mini_newsgroups/rec.motorcycles/105207,Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!darwin.sura.net!howland.reston.ans.net!usc!news.service.uci.edu!cerritos.edu!tanner\n",
      "From: tanner@cerritos.edu\n",
      "Newsgroups: rec.motorcycles\n",
      "Subject: Re: Posted Gif of BMW R100S\n",
      "Message-ID: <1993Apr26.020339.8132@cerritos.edu>\n",
      "Date: 26 Apr 93 02:03:39 PST\n",
      "References: <1993Apr22.201652.17882@news.columbia.edu>\n",
      "Organization: Cerritos College, Norwalk CA\n",
      "Lines: 26\n",
      "\n",
      "> \tIf any would care to see any more close-ups or different angles, I can\n",
      "> \tpost others to a.b.p also. I would be happy to submit one to cerritos\n",
      "> \tif someone wants to write me and tell me how...\n",
      "\n",
      "I would prefer a picture with you in it.  Since most motorcycles don't post,\n",
      "and are rather similar looking (i.e all R100S's are more alike than they are\n",
      "different), it is the people that are ultimately more interesting.\n",
      "\n",
      "From archive_policy.txt:\n",
      "> If you already have a picture in some machine-readable format (GIF preferred),\n",
      "> you can FTP it to Cerritos.edu account 'anonymous' password 'incoming', binary\n",
      "> mode and notify me by mail or mail it uuencoded to tanner@cerritos.edu.\n",
      ">\n",
      ">Please include a one or two line description for inclusion in AAAREADME.TXT.\n",
      "\n",
      "BTW, I have Charlie Smith's pictures available.\n",
      "\n",
      "> Sea-Bass Sears --> scs8@cunixb.cc.columbia.edu --> DoD#516 <-- |Stanley, ID.|\n",
      "\n",
      "Interestingly, I've been to Stanley, ID by motorcycle.  Nice little town,\n",
      "nice road getting there.\n",
      "\n",
      "-Bruce\n",
      "-- \n",
      "Bruce Tanner        (310) 860-2451 x 596    Tanner@Cerritos.EDU\n",
      "Cerritos College    Norwalk, CA             DoD #0161  NOMAD #007\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "newsgroupsRawData.takeSample(false, 1, 10L).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip out the filepath from the (filepath, content) pair\n",
    "Remember that wholeTextFiles returns (filepath, content) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val filepath = newsgroupsRawData.map{case(filepath,text) => (filepath)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at some sample filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s794-746ee4c5b0e6ab-a5f39cf201a0/notebook/work/mini_newsgroups/sci.electronics/54244\n",
      "file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s794-746ee4c5b0e6ab-a5f39cf201a0/notebook/work/mini_newsgroups/talk.politics.guns/55231\n",
      "file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s794-746ee4c5b0e6ab-a5f39cf201a0/notebook/work/mini_newsgroups/rec.motorcycles/104974\n",
      "file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s794-746ee4c5b0e6ab-a5f39cf201a0/notebook/work/mini_newsgroups/sci.crypt/15702\n",
      "file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/s794-746ee4c5b0e6ab-a5f39cf201a0/notebook/work/mini_newsgroups/talk.politics.misc/176982\n"
     ]
    }
   ],
   "source": [
    "filepath.takeSample(false, 5, 10L).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the document text from the (filepath, content) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val text = newsgroupsRawData.map{case(filepath,text) => text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate that just the text was extracted from the (filepath, content) pair\n",
    "Note that no filepath information is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: cantaloupe.srv.cs.cmu.edu!rochester!udel!darwin.sura.net!howland.reston.ans.net!usc!news.service.uci.edu!cerritos.edu!tanner\n",
      "From: tanner@cerritos.edu\n",
      "Newsgroups: rec.motorcycles\n",
      "Subject: Re: Posted Gif of BMW R100S\n",
      "Message-ID: <1993Apr26.020339.8132@cerritos.edu>\n",
      "Date: 26 Apr 93 02:03:39 PST\n",
      "References: <1993Apr22.201652.17882@news.columbia.edu>\n",
      "Organization: Cerritos College, Norwalk CA\n",
      "Lines: 26\n",
      "\n",
      "> \tIf any would care to see any more close-ups or different angles, I can\n",
      "> \tpost others to a.b.p also. I would be happy to submit one to cerritos\n",
      "> \tif someone wants to write me and tell me how...\n",
      "\n",
      "I would prefer a picture with you in it.  Since most motorcycles don't post,\n",
      "and are rather similar looking (i.e all R100S's are more alike than they are\n",
      "different), it is the people that are ultimately more interesting.\n",
      "\n",
      "From archive_policy.txt:\n",
      "> If you already have a picture in some machine-readable format (GIF preferred),\n",
      "> you can FTP it to Cerritos.edu account 'anonymous' password 'incoming', binary\n",
      "> mode and notify me by mail or mail it uuencoded to tanner@cerritos.edu.\n",
      ">\n",
      ">Please include a one or two line description for inclusion in AAAREADME.TXT.\n",
      "\n",
      "BTW, I have Charlie Smith's pictures available.\n",
      "\n",
      "> Sea-Bass Sears --> scs8@cunixb.cc.columbia.edu --> DoD#516 <-- |Stanley, ID.|\n",
      "\n",
      "Interestingly, I've been to Stanley, ID by motorcycle.  Nice little town,\n",
      "nice road getting there.\n",
      "\n",
      "-Bruce\n",
      "-- \n",
      "Bruce Tanner        (310) 860-2451 x 596    Tanner@Cerritos.EDU\n",
      "Cerritos College    Norwalk, CA             DoD #0161  NOMAD #007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.takeSample(false, 1, 10L).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the filename from the full filepath\n",
    "For example, extract the filename '54200' from the full filepath of '/resources/data/mini_newsgroups/talk.politics.guns/54200'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val id = filepath.map(filepath => (filepath.split(\"/\").takeRight(1))(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show that filenames have been extracted from the full filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53633\n",
      "54244\n",
      "53150\n",
      "54237\n",
      "53490\n"
     ]
    }
   ],
   "source": [
    "id.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the topic from the filepath\n",
    "Documents in the data set are stored in directories according to topic. The lowest level directory represents the topic classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val topic = filepath.map (filepath => (filepath.split(\"/\").takeRight(2))(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate that topics were extracted from the filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.electronics\n",
      "rec.motorcycles\n",
      "comp.graphics\n",
      "alt.atheism\n",
      "comp.windows.x\n",
      "rec.sport.baseball\n",
      "talk.politics.mideast\n",
      "rec.autos\n",
      "talk.politics.guns\n",
      "misc.forsale\n",
      "sci.space\n",
      "sci.crypt\n",
      "comp.sys.ibm.pc.hardware\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n",
      "comp.sys.mac.hardware\n",
      "rec.sport.hockey\n",
      "comp.os.ms-windows.misc\n"
     ]
    }
   ],
   "source": [
    "topic.distinct().take(20).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put the data into a DataFrame\n",
    "#### Define a case class and convert to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: string, text: string, topic: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class newsgroupsCaseClass(id: String, text: String, topic: String)\n",
    "\n",
    "val newsgroups = newsgroupsRawData.map{case (filepath, text) => \n",
    "    val id = filepath.split(\"/\").takeRight(1)(0)\n",
    "    val topic = filepath.split(\"/\").takeRight(2)(0)\n",
    "    newsgroupsCaseClass(id, text, topic)}.toDF()\n",
    "newsgroups.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the DataFrame schema and display 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      "\n",
      "+------+--------------------+------------------+\n",
      "|    id|                text|             topic|\n",
      "+------+--------------------+------------------+\n",
      "| 54160|Path: cantaloupe....|       alt.atheism|\n",
      "|101624|Newsgroups: rec.a...|         rec.autos|\n",
      "|103667|Newsgroups: rec.a...|         rec.autos|\n",
      "| 55278|Xref: cantaloupe....|talk.politics.guns|\n",
      "| 53772|Newsgroups: sci.e...|   sci.electronics|\n",
      "+------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsgroups.printSchema()\n",
    "newsgroups.sample(false,0.005,10L).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show document count by topic\n",
    "##### using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               topic|count|\n",
      "+--------------------+-----+\n",
      "|    rec.sport.hockey|  100|\n",
      "|     sci.electronics|  100|\n",
      "|             sci.med|  100|\n",
      "|           rec.autos|  100|\n",
      "|comp.sys.mac.hard...|  100|\n",
      "|      comp.windows.x|  100|\n",
      "|  rec.sport.baseball|  100|\n",
      "|comp.sys.ibm.pc.h...|  100|\n",
      "|        misc.forsale|  100|\n",
      "|     rec.motorcycles|  100|\n",
      "|           sci.crypt|  100|\n",
      "|  talk.politics.misc|  100|\n",
      "|       comp.graphics|  100|\n",
      "|         alt.atheism|  100|\n",
      "|talk.politics.mid...|  100|\n",
      "|soc.religion.chri...|  100|\n",
      "|  talk.politics.guns|  100|\n",
      "|comp.os.ms-window...|  100|\n",
      "|           sci.space|  100|\n",
      "|  talk.religion.misc|  100|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsgroups.groupBy(\"topic\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show only documents that are related to computer topics\n",
    "##### That is have \"comp\" in the topic name\n",
    "###### Using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   id|                text|               topic|\n",
      "+-----+--------------------+--------------------+\n",
      "| 9519|Newsgroups: comp....|comp.os.ms-window...|\n",
      "| 9814|Xref: cantaloupe....|comp.os.ms-window...|\n",
      "|39078|Xref: cantaloupe....|       comp.graphics|\n",
      "|38839|Newsgroups: comp....|       comp.graphics|\n",
      "|38244|Newsgroups: comp....|       comp.graphics|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsgroups.filter(newsgroups(\"topic\").like(\"comp%\")).sample(false,0.01,10L).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "This demo will use a Spark MLlib Logistic Regression algorithm to classify the documents into topics. The Logistic Regression method requires a numeric label of type double and can not work directly with the text topic categories that we extracted from input dataset. As stated above, the goal of this exercise to to classify documents in terms of whether they are computer related or not. As we saw above, the documents that are computer related reside in directories that begin with \"comp\". What we are now going to do is assing a numeric column called 'label' with a value of 0 for all non-computer related documents (those without \"comp\" in the topic name) and a value of 1 for all computer related documents (those with \"comp\" in the topic name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val labelednewsgroups = newsgroups.withColumn(\"label\", newsgroups(\"topic\").like(\"comp%\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the label column\n",
    "###### Using the DataFrame API\n",
    "Label is set to 0 for non-computer related topics and 1 for computer related topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----+\n",
      "|   id|                text|               topic|label|\n",
      "+-----+--------------------+--------------------+-----+\n",
      "|55278|Xref: cantaloupe....|  talk.politics.guns|  0.0|\n",
      "|38839|Newsgroups: comp....|       comp.graphics|  1.0|\n",
      "|52403|Newsgroups: comp....|comp.sys.mac.hard...|  1.0|\n",
      "|51996|Path: cantaloupe....|comp.sys.mac.hard...|  1.0|\n",
      "|83717|Xref: cantaloupe....|  talk.religion.misc|  0.0|\n",
      "+-----+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+-----+\n",
      "|   id|                text|               topic|label|\n",
      "+-----+--------------------+--------------------+-----+\n",
      "| 9519|Newsgroups: comp....|comp.os.ms-window...|  1.0|\n",
      "| 9814|Xref: cantaloupe....|comp.os.ms-window...|  1.0|\n",
      "|39078|Xref: cantaloupe....|       comp.graphics|  1.0|\n",
      "|38839|Newsgroups: comp....|       comp.graphics|  1.0|\n",
      "|38244|Newsgroups: comp....|       comp.graphics|  1.0|\n",
      "+-----+--------------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelednewsgroups.sample(false,0.003,10L).show(5)\n",
    "labelednewsgroups.filter(newsgroups(\"topic\").like(\"comp%\")).sample(false,0.007,10L).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data set into separate training (90%) and test (10%) data sets\n",
    "#### Split documents from a list of (id, text, label) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val Array(training, test) = labelednewsgroups.randomSplit(Array(0.9, 0.1), seed = 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show a count of the resulting training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Document Count = 2000\n",
      "Training Count = 1777, 88.85%\n",
      "Test Count = 223, 11.15%\n"
     ]
    }
   ],
   "source": [
    "println(\"Total Document Count = \" + labelednewsgroups.count())\n",
    "println(\"Training Count = \" + training.count() + \", \" + training.count*100/(labelednewsgroups.count()).toDouble + \"%\")\n",
    "println(\"Test Count = \" + test.count() + \", \" + test.count*100/(labelednewsgroups.count().toDouble) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an ML Pipeline\n",
    "In machine learning, it is common to run a sequence of algorithms to process and learn from data. Spark ML represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages (Transformers and Estimators) to be run in a specific order. The pipeline we are using in this example consists of five stages: Tokenizer, StopWordsRemover, HashingTF, Inverse Document Frequency (IDF) and LogisticRegression.\n",
    "\n",
    "**Tokenizer** splits the raw text documents into words, adding a new column with words into the dataset.\n",
    "\n",
    "**StopWordsRemover** takes as input a sequence of strings and drops all the stop words from the input sequences. Stop words are words which should be excluded from the input, typically because the words appear frequently and don’t carry as much meaning. A list of stop words by default. Optionally you can provide a list of stopwords. We will just use the defualt list of stopwords.\n",
    "\n",
    "**HashingTF** takes sets of terms and converts those sets into fixed-length feature vectors. \n",
    "\n",
    "**Inverse Document Frequency (IDF)** is a numerical measure of how much information a term provides. If a term appears very often across the corpus, it means it doesn’t carry special information about a particular document. IDF down-weights terms which appear frequently in a corpus.\n",
    "\n",
    "**LogisticRegression** is a method used to predict a binary response. The current implementation of logistic regression in spark.ml only supports binary classes. Support for multiclass regression will be added in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "val remover = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(false)\n",
    "val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n",
    "val lr = new LogisticRegression().setRegParam(0.01).setThreshold(0.5)\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, remover, hashingTF, idf, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Logistic Regression Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Features Column = features\n",
      "Logistic Regression Label Column = label\n",
      "Logistic Regression Threshold = 0.5\n"
     ]
    }
   ],
   "source": [
    "println(\"Logistic Regression Features Column = \" + lr.getFeaturesCol)\n",
    "println(\"Logistic Regression Label Column = \" + lr.getLabelCol)\n",
    "println(\"Logistic Regression Threshold = \" + lr.getThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the parameters associated with each stage in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer:\n",
      "inputCol: input column name (current: text)\n",
      "outputCol: output column name (default: tok_fe57090347cf__output, current: words)\n",
      "*************************\n",
      "Remover:\n",
      "caseSensitive: whether to do case-sensitive comparison during filtering (default: false, current: false)\n",
      "inputCol: input column name (current: words)\n",
      "outputCol: output column name (default: stopWords_4ce720b2751c__output, current: filtered)\n",
      "stopWords: stop words (default: [Ljava.lang.String;@6dafc8db)\n",
      "*************************\n",
      "HashingTF:\n",
      "inputCol: input column name (current: filtered)\n",
      "numFeatures: number of features (> 0) (default: 262144, current: 1000)\n",
      "outputCol: output column name (default: hashingTF_8562bb6678ac__output, current: rawFeatures)\n",
      "*************************\n",
      "IDF:\n",
      "inputCol: input column name (current: rawFeatures)\n",
      "minDocFreq: minimum of documents in which a term should appear for filtering (default: 0, current: 0)\n",
      "outputCol: output column name (default: idf_9f6372dcf662__output, current: features)\n",
      "*************************\n",
      "LogisticRegression:\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5, current: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values >= 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class' threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (default: 1.0E-6)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (default: )\n",
      "*************************\n",
      "Pipeline:\n",
      "stages: stages of the pipeline (current: [Lorg.apache.spark.ml.PipelineStage;@8feeba96)\n"
     ]
    }
   ],
   "source": [
    "println(\"Tokenizer:\")\n",
    "println(tokenizer.explainParams())\n",
    "println(\"*************************\")\n",
    "println(\"Remover:\")\n",
    "println(remover.explainParams())\n",
    "println(\"*************************\")\n",
    "println(\"HashingTF:\")\n",
    "println(hashingTF.explainParams())\n",
    "println(\"*************************\")\n",
    "println(\"IDF:\")\n",
    "println(idf.explainParams())\n",
    "println(\"*************************\")\n",
    "println(\"LogisticRegression:\")\n",
    "println(lr.explainParams())\n",
    "println(\"*************************\")\n",
    "println(\"Pipeline:\")\n",
    "println(pipeline.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the list of default Stop Words that were applied in the pipeline\n",
    "Stop Words are words which should be excluded from the input, typically because the words appear frequently and don’t carry as much meaning. You may also optionally provide your own list of Stop Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "about\n",
      "above\n",
      "across\n",
      "after\n",
      "afterwards\n",
      "again\n",
      "against\n",
      "all\n",
      "almost\n",
      "alone\n",
      "along\n",
      "already\n",
      "also\n",
      "although\n",
      "always\n",
      "am\n",
      "among\n",
      "amongst\n",
      "amoungst\n",
      "amount\n",
      "an\n",
      "and\n",
      "another\n",
      "any\n",
      "anyhow\n",
      "anyone\n",
      "anything\n",
      "anyway\n",
      "anywhere\n",
      "are\n",
      "around\n",
      "as\n",
      "at\n",
      "back\n",
      "be\n",
      "became\n",
      "because\n",
      "become\n",
      "becomes\n",
      "becoming\n",
      "been\n",
      "before\n",
      "beforehand\n",
      "behind\n",
      "being\n",
      "below\n",
      "beside\n",
      "besides\n",
      "between\n",
      "beyond\n",
      "bill\n",
      "both\n",
      "bottom\n",
      "but\n",
      "by\n",
      "call\n",
      "can\n",
      "cannot\n",
      "cant\n",
      "co\n",
      "con\n",
      "could\n",
      "couldnt\n",
      "cry\n",
      "de\n",
      "describe\n",
      "detail\n",
      "do\n",
      "done\n",
      "down\n",
      "due\n",
      "during\n",
      "each\n",
      "eg\n",
      "eight\n",
      "either\n",
      "eleven\n",
      "else\n",
      "elsewhere\n",
      "empty\n",
      "enough\n",
      "etc\n",
      "even\n",
      "ever\n",
      "every\n",
      "everyone\n",
      "everything\n",
      "everywhere\n",
      "except\n",
      "few\n",
      "fifteen\n",
      "fify\n",
      "fill\n",
      "find\n",
      "fire\n",
      "first\n",
      "five\n",
      "for\n",
      "former\n",
      "formerly\n",
      "forty\n",
      "found\n",
      "four\n",
      "from\n",
      "front\n",
      "full\n",
      "further\n",
      "get\n",
      "give\n",
      "go\n",
      "had\n",
      "has\n",
      "hasnt\n",
      "have\n",
      "he\n",
      "hence\n",
      "her\n",
      "here\n",
      "hereafter\n",
      "hereby\n",
      "herein\n",
      "hereupon\n",
      "hers\n",
      "herself\n",
      "him\n",
      "himself\n",
      "his\n",
      "how\n",
      "however\n",
      "hundred\n",
      "i\n",
      "ie\n",
      "if\n",
      "in\n",
      "inc\n",
      "indeed\n",
      "interest\n",
      "into\n",
      "is\n",
      "it\n",
      "its\n",
      "itself\n",
      "keep\n",
      "last\n",
      "latter\n",
      "latterly\n",
      "least\n",
      "less\n",
      "ltd\n",
      "made\n",
      "many\n",
      "may\n",
      "me\n",
      "meanwhile\n",
      "might\n",
      "mill\n",
      "mine\n",
      "more\n",
      "moreover\n",
      "most\n",
      "mostly\n",
      "move\n",
      "much\n",
      "must\n",
      "my\n",
      "myself\n",
      "name\n",
      "namely\n",
      "neither\n",
      "never\n",
      "nevertheless\n",
      "next\n",
      "nine\n",
      "no\n",
      "nobody\n",
      "none\n",
      "noone\n",
      "nor\n",
      "not\n",
      "nothing\n",
      "now\n",
      "nowhere\n",
      "of\n",
      "off\n",
      "often\n",
      "on\n",
      "once\n",
      "one\n",
      "only\n",
      "onto\n",
      "or\n",
      "other\n",
      "others\n",
      "otherwise\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "out\n",
      "over\n",
      "own\n",
      "part\n",
      "per\n",
      "perhaps\n",
      "please\n",
      "put\n",
      "rather\n",
      "re\n",
      "same\n",
      "see\n",
      "seem\n",
      "seemed\n",
      "seeming\n",
      "seems\n",
      "serious\n",
      "several\n",
      "she\n",
      "should\n",
      "show\n",
      "side\n",
      "since\n",
      "sincere\n",
      "six\n",
      "sixty\n",
      "so\n",
      "some\n",
      "somehow\n",
      "someone\n",
      "something\n",
      "sometime\n",
      "sometimes\n",
      "somewhere\n",
      "still\n",
      "such\n",
      "system\n",
      "take\n",
      "ten\n",
      "than\n",
      "that\n",
      "the\n",
      "their\n",
      "them\n",
      "themselves\n",
      "then\n",
      "thence\n",
      "there\n",
      "thereafter\n",
      "thereby\n",
      "therefore\n",
      "therein\n",
      "thereupon\n",
      "these\n",
      "they\n",
      "thick\n",
      "thin\n",
      "third\n",
      "this\n",
      "those\n",
      "though\n",
      "three\n",
      "through\n",
      "throughout\n",
      "thru\n",
      "thus\n",
      "to\n",
      "together\n",
      "too\n",
      "top\n",
      "toward\n",
      "towards\n",
      "twelve\n",
      "twenty\n",
      "two\n",
      "un\n",
      "under\n",
      "until\n",
      "up\n",
      "upon\n",
      "us\n",
      "very\n",
      "via\n",
      "was\n",
      "we\n",
      "well\n",
      "were\n",
      "what\n",
      "whatever\n",
      "when\n",
      "whence\n",
      "whenever\n",
      "where\n",
      "whereafter\n",
      "whereas\n",
      "whereby\n",
      "wherein\n",
      "whereupon\n",
      "wherever\n",
      "whether\n",
      "which\n",
      "while\n",
      "whither\n",
      "who\n",
      "whoever\n",
      "whole\n",
      "whom\n",
      "whose\n",
      "why\n",
      "will\n",
      "with\n",
      "within\n",
      "without\n",
      "would\n",
      "yet\n",
      "you\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n"
     ]
    }
   ],
   "source": [
    "remover.getStopWords.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the pipeline to the training documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on document in the Test data set\n",
    "#### Keep in mind that the model has not seen the documents in the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model against the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show results using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|   id|               topic|         probability|prediction|label|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|54446|  talk.politics.guns|[0.85907386640572...|       0.0|  0.0|\n",
      "|53911|     sci.electronics|[0.99375677040263...|       0.0|  0.0|\n",
      "|67516|      comp.windows.x|[0.78541867608477...|       0.0|  1.0|\n",
      "|51539|comp.sys.mac.hard...|[0.99035383473721...|       0.0|  1.0|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|   id|               topic|         probability|prediction|label|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "| 9151|comp.os.ms-window...|[0.99677353549827...|       0.0|  1.0|\n",
      "|38942|       comp.graphics|[0.41936760457692...|       1.0|  1.0|\n",
      "|51613|comp.sys.mac.hard...|[0.07059632616872...|       1.0|  1.0|\n",
      "|60199|comp.sys.ibm.pc.h...|[0.01268251597292...|       1.0|  1.0|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"id\", \"topic\", \"probability\", \"prediction\", \"label\").sample(false,0.01,10L).show(5)\n",
    "predictions.select(\"id\", \"topic\", \"probability\", \"prediction\", \"label\").filter(predictions(\"topic\").like(\"comp%\")).sample(false,0.1,10L).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show all the fields in order to see the results of each stage in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.sample(false,0.001,10L).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an evaluator for the binary classification\n",
    "In this example we will use area under the ROC curve as the evaluation metric. Receiver operating characteristic (ROC) is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied. The curve is created by plotting the true positive rate against the false positive rate at various threshold settings. The ROC curve is thus the sensitivity as a function of fall-out. The area under the ROC curve is useful for comparing and selecting the best machine learning model for a given data set. A model with an area under the ROC curve score near 1 has very good performance. A model with a score near 0.5 is about as good as flipping a coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve = 0.8736702127659571\n"
     ]
    }
   ],
   "source": [
    "val evaluator = new BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "println(\"Area under the ROC curve = \" + evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters\n",
    "#### Generate hyperparameter combinations by taking the cross product of some parameter values\n",
    "\n",
    "Spark MLlib algorithms provide many hyperparameters for tuning models. These hyperparameters are distinct from the model parameters being optimized by MLlib itself. Hyperparameter tuning is accomplished by choosing the best set of parameters based on model performance on test data that the model was not trained with. All combinations of hyperparameters specified will be tried in order to find the one that leads to the model with the best evaluation result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Parameter Grid specifying what parameters and values will be evaluated in order to determine the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val paramGrid = new ParamGridBuilder().\n",
    "  //addGrid(hashingTF.numFeatures, Array(1000, 10000, 100000)).\n",
    "  //addGrid(idf.minDocFreq, Array(0,10, 100)).\n",
    "  addGrid(lr.regParam, Array(0.01, 0.1, 0.2)).\n",
    "  addGrid(lr.threshold, Array(0.5, 0.6, 0.7)).\n",
    "  build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cross validator to tune the pipeline with the generated parameter grid\n",
    "Spark MLlib provides for cross-validation for hyperparameter tuning. Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluate the ML Pipeline to find the best model\n",
    "using the area under the ROC evaluator and hyperparameters specified in the parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve for best fitted model = 0.8629110251450667\n"
     ]
    }
   ],
   "source": [
    "val cvModel = cv.fit(training)\n",
    "println(\"Area under the ROC curve for best fitted model = \" + evaluator.evaluate(cvModel.transform(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see what improvement we achieve by tuning the hyperparameters using cross-evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve for non-tuned model = 0.8736702127659574\n",
      "Area under the ROC curve for fitted model = 0.8629110251450667\n",
      "Improvement = -1.23%\n"
     ]
    }
   ],
   "source": [
    "println(\"Area under the ROC curve for non-tuned model = \" + evaluator.evaluate(predictions))\n",
    "println(\"Area under the ROC curve for fitted model = \" + evaluator.evaluate(cvModel.transform(test)))\n",
    "println(\"Improvement = \" + \"%.2f\".format((evaluator.evaluate(cvModel.transform(test)) - evaluator.evaluate(predictions)) *100 / evaluator.evaluate(predictions)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:101: error: value stages is not a member of org.apache.spark.ml.Model[_$4]\n",
       "              cvModel.bestModel.stages(4)\n",
       "                                ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.8676415844070431, 0.86764158440704, 0.8676415844070429, 0.8802002011679027, 0.8802002011679031, 0.8802002011679044, 0.8799315803705203, 0.8799315803705223, 0.8799315803705212)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make improved predictions on documents using the Cross-validated model\n",
    "#### using the Test data set\n",
    "Using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|   id|               topic|         probability|prediction|label|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|54724|    rec.sport.hockey|[0.98745011905507...|       0.0|  0.0|\n",
      "|61180|           sci.space|[0.99146856892132...|       0.0|  0.0|\n",
      "|67320|      comp.windows.x|[0.32659495149272...|       1.0|  1.0|\n",
      "|52300|comp.sys.mac.hard...|[0.02470669961662...|       1.0|  1.0|\n",
      "|83827|  talk.religion.misc|[0.95422472487723...|       0.0|  0.0|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel.transform(test).select(\"id\", \"topic\", \"probability\", \"prediction\", \"label\").sample(false,0.01,0L).show(5)\n",
    "cvModel.transform(test).select(\"id\", \"topic\", \"probability\", \"prediction\", \"label\").filter(predictions(\"topic\").like(\"comp%\")).sample(false,0.1,0L).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "#### This analysis was intended to illustrate how to use the spark.ml machine learning package utilizing a machine learning pipeline. Although a document classification use case was specifically demonstrated, many of the principles demonstrated in the notebook can be employed to other machine learning use cases. Obviously the algorithms that need to be employed will be dependent on the specific use case.\n",
    "\n",
    "#### Also please note that although this demo illustrated how to tune a model for best fit, no attempt was made to actually optimize to the best possible model. The intent was simply to show the methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}